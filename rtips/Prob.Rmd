---
title: "Calculate probability and likelihood"
author:

<!--  output: html_document -->
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: 
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

R has many built-in functions to calculate probabilities for discrete random variables and probability densities for continuous random variables. These are additionally useful for calculating likelihoods. This section highlights a few of the most commonly used probability distributions. If used to calculate likelihoods, the <code>log=TRUE</code> option (yielding log likelihoods instead) is recommended to prevent memory overflow or underflow.

The basic structure of these commands is detailed in the section below on the binomial distribution. The structure of other commands will be similar, though options may vary.

***

## Binomial distribution
Used for binary data (1 or 0; success or failure; alive or dead; mated or unmated). The number of "successes" in <i>n</i> trials will have a binomial distribution if separate trials are independent and if the probability of success <i>p</i> is the same in every trial.

In its simplest version, the "dbinom" command is used to calculate Pr[<i>X</i>], the probability of obtaining <i>X</i> successes in <i>n</i> random trials, where <i>X</i> is an integer between 0 and <i>n</i> (<i>n</i> can be as small as 1 trial). To calculate this probability you will also need to specify <i>p</i>, the probability of success in each trial,

```{r eval = FALSE}
dbinom(X, size = n, prob = p)
```

The same command calculates <i>L</i>[ <i>p </i>| <i>X </i>], the likelihood of the parameter value <i>p</i> given the observed number of successes <i>X</i>,

```{r eval = FALSE}
dbinom(X, size = n, prob = p)              # likelihood of p
dbinom(X, size = n, prob = p, log = TRUE)  # log-likelihood of p
```

If <i>X</i> is a single number (rather than a vector), the same command can be used to obtain the log-likelihood of each of many values for <i>p</i>. For example,

```{r eval = FALSE}
p <- seq(0.01, 0.99, by = 0.01)
loglike <- dbinom(X, size = n, prob = p ,log = TRUE)
```

However, the data more typically come as a vector of measurements made on individuals rather than a single count. For example, the variable "disease.state" in a data frame would have entries that looked something like
<code>id disease.state
1      infected
2    uninfected
3      infected
4      infected
5    uninfected
...</code>
In this case, to calculate the log-likelihood of a specified value for <i>p</i> (the probability that an individual in the population is infected) you will first need to calculate the log-likelihood of <i>p</i> for each data observation (0 = uninfected, 1 = infected). The log-likelihood for <i>p</i> given all the data is then the sum of the log-likelihoods based on each observation. This assumes that the data represent a random sample (so that "trials" are independent).

```{r eval = FALSE}
x <- as.integer(disease.state == "infected")    # converts to 0 and 1's
z <- dbinom(x, size = 1, prob = p, log = TRUE)  # log-like for each obs
loglike <-sum(z)                                # log-likelihood of p
```

(Note: If you calculate the likelihood of <i>p</i> for each data observation, rather than the log-likelihood, then the likelihood of <i>p</i> given all the data will be the product of the likelihoods based on each observation, rather than their sum. This number can get very small, however, so calculating the log-likelihoods and summing is recommended.)

***


## Geometric distribution
In a sequence of trials, the number of trials ending in failure before the first success has a geometric distribution. Each trial in the sequence is assumed to be independent. The outcome of each trial is binary, either "success" or "failure". The probability of success <i>p</i> is the same in every trial. The probability of failure in each trial is 1 - <i>p</i>.

The variable <i>X</i> counts the number of "failures" before the first "success". In other words, <i>X</i> = 0 if success occurs on the 1st trial. <i>X</i> = 1 means that the first success occurred in the 2nd trial and the 1st trial ended in failure. <i>X</i> = 2 means that the that success occurred in the 3rd trial, while the 1st and 2nd trials ended in failure. And so on. Pr[<i>X</i>] is calculated as

```{r eval = FALSE}
dgeom(X, prob = p)            # p is the probability of success
dgeom(X, prob = p, log=TRUE)  # log of the probability instead
```


***


## Hypergeometric distribution
Used for mark-recapture data. The hypergeometric distribution describes the number of recaptures (marked individuals) <i>X</i> in a random sample of <i>k</i> individuals from a finite population in which <i>m</i> individuals in total are marked and <i>n</i> individuals are unmarked.

The distribution is used to model mark-recapture data in which the goal is to estimate total population size <i>m</i> + <i>n</i>, where <i>n</i> is unknown. <i>X</i> can be any integer between 0 and <i>m</i>. <i>k</i> can be any integer between 1 and <i>m</i> + <i>n</i>. The model assumes no births, deaths, or immigration or emigration events between marking and recapturing. It also assumes that all individuals in the population have the same probability of being captured. The probability of <i>X</i> recaptures (marked individuals) in a random sample of size <i>k</i> is

```{r eval = FALSE}
dhyper(X, m, n, k)
dhyper(X, m, n, k, log = TRUE)
```


***


## Poisson distribution
Used for count data. The Poisson distribution describes the number of events <i>X</i> in a "block" of space or time. The single parameter <i>lambda</i> is the population mean number of events per block. The assumption is that individual events occur randomly and independently in space or time. Block size is arbitrary but is usually chosen such that the mean number of events per block is neither very small nor very large. The probability of <i>X</i> events occurring in a single block is

```{r eval = FALSE}
dpois(X, lambda)
dpois(X, lambda, log = TRUE)
```

***

## Normal distribution
Optimistically, this probability distribution approximates the frequency distribution of a trait in a population (except when it doesn't!). Because it is a continuous distribution, the height of the normal curve refers to <b>probability density</b> rather than probability as such. Probability is instead represented by area under the normal curve.

The probability density of a normally distributed variable <span style="font-style: italic;">X</span> having mean µ and standard deviation σ is

```{r eval = FALSE}
dnorm(X, mean = µ, sd = σ)
dnorm(X, mean = µ, sd = σ, log = TRUE)
```

***

## Exponential distribution
The exponential distribution is often used to model the waiting time <i>X</i> between events occurring randomly and independently in time (or space). Because it is a continuous distribution, the height of the exponential curve at any <i>X</i> refers to <b>probability density</b> rather than probability. Probability is instead represented by area under the exponential curve.

The main assumption of the exponential distribution is that at any point in time, the probability of an event occurring in the next instant does not depend on how much time has already elapsed since the previous event. The parameter of the exponential distribution is the <b>rate</b> at which events occur -- the number of events per unit time. The mean of the exponential distribution is 1/rate.

Waiting time <i>X</i> to the next event has probability density

```{r eval = FALSE}
dexp(X, rate = 1)
dexp(X, rate = 1, log = TRUE)
```

The default value for the rate is 1, so you must alter to fit your circumstance. Remember: the rate is the inverse of the mean


***

## Use the ```bbmle()``` package

The ```bbmle()``` package can quickly find the maximum likelihood estimate for a parameter of a probability distribution fitted to data. You might need to install the package the first time you use it. Then load the package to begin. 

```{r eval = FALSE}
library(bbmle)
```

The first step is to make a function that calculates the **negative** log likelihood of a parameter fitted to data. 

<br>

### Binomial distribution

For example, if fitting the binomial distribution, the function would look like the following. In this example, ```p``` is the unknown parameter to be estimated. ```X``` is the observed number of successes in the data and ```n``` is the observed number of trials (replace ```X``` and ```n``` in the command below with the actual numbers from the data).

```{r eval = FALSE}
myNegLLfunc <- function(p){-dbinom(X, size = n, p, log = TRUE)}
```

Notice the minus sign in front of ```dbinom```. The ```bbmle()``` package minimizes the negative log likelihood rather than maximizes the log likelihood, but the result is the same.

The next step is to fit the model to the data using the ```mle2()``` command The argument ```start = list(p = 0.5)``` provides a realistic starting value to begin the search for the maximum likelihood estimate. When fitting a single parameter, the profile log-likelihood is the same as the log-likelihood curve.

```{r eval = FALSE}
z <- mle2(myNegLLfunc, start = list(p = 0.5))
```

Finally, we apply functions to the model object ```z``` to extract useful quantities. 

```{r eval = FALSE}
summary(z)                            # Provides the MLE estimate. Ignore the P-value, as usual.
plot(profile(z))                      # Visualize CI's for the parameter at different alpha levels.
confint(profile(z), method="uniroot") # Calculate the 95% confidence interval.
```

<br>

### Exponential distribution

This second example shows how you would incude data from a data frame in the function for the negative log likelihood. The exponential distribution is used to model waiting times to an events. The parameter to be estimated is the ```rate```. The data on waiting times is assumed to be a column ```t``` in a data frame ```mydata```.

```{r eval = FALSE}
myNegLLfunc <- function(rate){-sum(dexp(mydata$t, rate = rate, log = TRUE))}
```

The rest of the analysis is idetical to that shown above for the binomial distribution
