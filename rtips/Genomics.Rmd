---
title: "Genomics"
author:

<!--  output: html_document -->
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: 
      collapsed: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

These pages describe R-assisted methods we use to analyze stickleback genome sequences on the Cedar or Graham supercomputers. The sequence of steps is based on Felicity Jones' lab pipeline.

Graham uses <code>slurm</code> to schedule jobs, and it uses modules to load programs as needed. Commands here ignore paths to files in other folders and assumes that all necessary files are available in your current working directory. Modify as needed.

Compute Canada keeps track of how efficiently you use its resources. If you are given a chunk of time and memory (e.g., in interactive mode or batch job) and fail to use it, your priority rating goes down (future jobs will sit longer in the queue). So it is a good idea to request the minimum amount that you need, and to exit from interactive mode as soon as your test run is completed. See the section below on [monitoring jobs](#monitor) to figure out what resources you actually used.

***

<!-- ========================================================================= -->

## Run programs on Graham
To get started, type the following into a command window on your local machine. Replace "schluter" with your name to log into your own account. Most work will be done in the <code>scratch</code> folder, which is not backed up but has the most disc space.

```{r eval = FALSE}
ssh schluter@graham.computecanada.ca  # login
cd ~/scratch/stk                      # change to working folder
```

Simple tasks not requiring much memory or time can be run from the command line (e.g., small R tasks).

<br>

### Software

A full list of software is <a href="https://docs.computecanada.ca/wiki/Available_software">here</a>.

Running software requires that you first load the corresponding module, indicating also the version number. For example, the next section shows how to run R by first loading the module. 

<br>

### Run R

I use R to do a lot of the overhead associated with writing scripts and repeating jobs on many files. To run R on Graham, load the module first. Then type <code>R</code> to start a command line. My <code>module</code> command below also loads Bioconductor and a prerequisite C compiler.

```{r eval = FALSE}
module load gcc/7.3.0 nixpkgs/16.09 r/3.6.0 r-bundle-bioconductor/3.9
R
```

Many R packages, including Bioconductor, are already installed on Compute Canada systems. To check which ones are available to you, type <code>library()</code> on the R command line.

Additional packages can be installed in your local folder. For example:

```{r eval = FALSE}
# in R
install.packages("hierfstat")
install.packages("WhopGenome")

# Here's one Bioconductor package you might need - the stickleback genome
BiocManager::install("BSgenome.Gaculeatus.UCSC.gasAcu1", update = FALSE, ask=FALSE) 
```

My homemade R function, <code>g$slurm()</code>, will create bash script files for you to put in the job queue when running in batch mode. This and other R functions are in a file named <code>genome.r</code> on my Github.

Start R on Graham and paste the following code into your R command window. (This won't work in interactive mode or inside a batch script file because Graham's job nodes are not connected to the internet for security reasons.)

```{r eval = FALSE}
# in R
git <- function(githubfile){
	library(RCurl)
	script <- getURL( paste("https://raw.githubusercontent.com/dschluter/genomeScripts/master/",
	            githubfile, sep="") )
	eval(parse(text = script), envir = .GlobalEnv)
	}
git("genome.r")
git("misc.r")
```

<br>

### Other things to learn

At the bottom of this page I've made notes on [other useful procedures](#other) for getting programs up and running on Cedar or Graham. This includes using [interactive mode](#interactive) to test code before you run a big job, [parallel processing](#parallel) to get things done faster, and more information on [submitting jobs](#submit), [monitoring jobs](#monitor), and figuring out the resources a job used after it finished.

***

<!-- ========================================================================= -->

## Prepare reference genome

You'll need the reference genome to begin. Ask Dolph for the files if you don't yet have them. We use the original Broad Institute genome assembly from [Jones et al (2012 Nature)](https://doi.org/10.1038/nature10944). At the end we convert to the [Glazer et al (2015 G3)](https://doi.org/10.1534/g3.115.017905) re-assembly coordinates.

The file ```gasAcu1.fa.gz``` is a gzipped fasta file containing the published Broad Institute assembly of the stickleback genome. It has 21 assembled chromosomes plus "chrUn", which is an artificial chromosome consisting of unassembled contigs chained together (individual contigs are separated by a large number of NNNNNNNNNs). Use the unix command "zless gasAcu1.fa.gz" to peek at the fasta file.

The file ```chrVIIpitx1new.fa``` is an unzipped fasta file from Felicity Jones containing sequence of the Pitx1 region of chromosome 7 from Salmon River, a region missing from the published genome sequence. 

<br>

### Start interactive R session

You can just run R from the command line once you've logged in. This will allow you to do basic R operations but nothing demanding lots of 
```{r eval = FALSE}
module load gcc/7.3.0 nixpkgs/16.09 r/3.6.0 r-bundle-bioconductor/3.9
R
```

But if you need lots of memory, consider running R inside a timed interaction session. But your work had better be done when your time runs out.

```{r eval = FALSE}
# in Unix:

# Check which directory you are in

pwd

# Request resources (1 hour, 1 core, 8Gb memory)
# (copy job allocation number down for later use)

salloc --time=1:0:0 --ntasks-per-node=1 --mem-per-cpu=8G --account=def-schluter

# Load modules and run R

module load gcc/7.3.0 nixpkgs/16.09 r/3.6.0 r-bundle-bioconductor/3.9
R
```

<br>

### Make fasta file

I've already done these steps, ask me for the files, or redo yourself for fun.

We want to add the Pitx1 region to the full assembly fasta file and save to a new genome fasta file named <code>gasAcu1pitx1.fa</code>. I used the ```seqinr``` package, which is slow but retains lower and upper case letters in the reference genome.  

```{r eval = FALSE}
# in R:

library(seqinr)

# Read fasta files to different objects

z <- read.fasta(file = "gasAcu1.fa.gz", seqtype = "DNA", 
          as.string = TRUE, forceDNAtolower = FALSE)
y <- read.fasta(file = "chrVIIpitx1new.fa", seqtype = "DNA", 
          as.string = TRUE, forceDNAtolower = FALSE)

# Combine pitx1 fragment with rest of genome

comboGenome <- c(z, y)

# Contents of combined genome

names(comboGenome)
	 # [1] "chrI"           "chrII"          "chrIII"         "chrIV"         
	 # [5] "chrIX"          "chrUn"          "chrV"           "chrVI"         
	 # [9] "chrVII"         "chrVIII"        "chrX"           "chrXI"         
	# [13] "chrXII"         "chrXIII"        "chrXIV"         "chrXIX"        
	# [17] "chrXV"          "chrXVI"         "chrXVII"        "chrXVIII"      
	# [21] "chrXX"          "chrXXI"         "chrM"           "chrVIIpitx1new"

# Save combined genome

write.fasta(sequences = comboGenome, names = names(comboGenome), 
        file.out = "gasAcu1pitx1.fa")
```
<!-- 
# Write individual fasta files
for(i in names(comboGenome)) write.fasta(sequences = comboGenome[i], 
        names = i, file.out = paste0(i, ".fa"))
-->

<br>

### Record masked bases

The stickleback reference genome was run through RepeatMasker, a program to find interspersed repeats and low complexity DNA sequences. The output is in the file ```gasAcu1.Feb2006.RepeatMasker.open4.0.5-RepeatLibrary20140131.out.gz```, which I downloaded from [www.repeatmasker.org](www.repeatmasker.org). The file lists the start and end position all the identified repeats. These are also indicated in the reference genome by lower case letters. 

THe code below reads the repeats from the file and saves them as an R object for later use. 

```{r eval = FALSE}

# Read file into a data frame x

x <- read.table(gzfile("GCF_002872995.1_Otsh_v1.0_rm.out.gz"), skip = 3, fill = TRUE, 
                stringsAsFactors = FALSE)

# Retain only the three most important columns

x <- x[, 5:7]
names(x) <- c("chr", "start", "end")

# Convert to a list, where each list element is a different chromosome

x <- split(x, x$chr)
repeatMaskedBases <- x[names(x) != ""]
length(repeatMaskedBases)
	# [1] 8520
save(repeatMaskedBases, file = "gasacu1.RepeatMaskedBases.rdd") 
```

<br>

### Make transcript database

R has tools to annotate sequence data. To use them, make a database of the stickleback annotation file located at Ensembl. 

```{r eval = FALSE}
library(GenomicFeatures)
library(rtracklayer)

gasaculEnsembl <- makeTxDbFromBiomart(biomart = "ensembl", dataset = "gaculeatus_gene_ensembl")
gasaculEnsembl

# Save it to a file named "gasaculEnsembl.sqlite" in the current directory

saveDb(gasaculEnsembl, file="gasaculEnsembl.sqlite")

# Later, to load the data base use (not run)
gasaculEnsembl <- loadDb("gasaculEnsembl.sqlite")
```

<br>

### Exit interactive job

It is **very important** to exit the interactive session if you are done before it expires. Idle resources and other wastage lowers your future priority in the job queue. Big Brother is watching you.

```{r eval = FALSE}
# in Unix

exit
```

<br>

### Check job resources

When a job is finished, use the following command to quantify the resources used by your job (my job number was 26645298). Exit code should be 0 if all went well. Check MaxRSS for total maximum memory used. Here's an example of the output, showing that I used a little over 6 Gb memory, not the full 8 requested. 

```{r eval = FALSE}

sacct --jobs=26645298 --format=JobID,JobName,AllocCPUS,ExitCode,MaxRSS,Elapsed,UserCPU

	       # JobID    JobName  AllocCPUS ExitCode     MaxRSS    Elapsed    UserCPU 
	# ------------ ---------- ---------- -------- ---------- ---------- ---------- 
	# 26645298             sh          1      0:0              00:15:39  02:50.047 
	# 26645298.ex+     extern          1      0:0       684K   00:15:39   00:00:00 
	# 26645298.0         bash          1      0:0   6044115K   00:15:34  02:50.047 
```

<br>

### Index fasta files

The genome files need to be indexed before using. This is fairly fast and could be done in an interactive session. Make sure that you have all the genome fasta (<code>*.fa</code>) files in the current directory.

Start by reserving resources in an interactive job. Here, 2 hours, 1 core, 2 Gb memory (little memory is needed for this task)

```{r eval = FALSE}
# in Unix:

salloc --time=2:0:0 --ntasks-per-node=1 --mem-per-cpu=2G --account=def-schluter
```

We'll need to run the following commands in unix (don't start yet - see below). Each command must be run on the whole genome fasta file and then on all the individual chromosome files as follows.

```{r eval = FALSE}
# Generate bwa index for running bwa mem
module load bwa/0.7.17
bwa index -a bwtsw gasAcu1pitx1.fa
bwa index -a bwtsw chrI.fa
bwa index -a bwtsw chrII.fa
bwa index -a bwtsw chrIII.fa
... etc

# Generate .fai index files needed by GATK
module load samtools/1.9
samtools faidx gasAcu1pitx1.fa
samtools faidx chrI.fa
samtools faidx chrII.fa
samtools faidx chrIII.fa
... etc

# Generate .dict files needed by GATK
module load picard/2.20.6
java -Xmx2g -jar $EBROOTPICARD/picard.jar CreateSequenceDictionary R=gasAcu1pitx1.fa 
    O=gasAcu1pitx1new.dict
java -Xmx2g -jar $EBROOTPICARD/picard.jar CreateSequenceDictionary R=chrI.fa O=chrI.dict
java -Xmx2g -jar $EBROOTPICARD/picard.jar CreateSequenceDictionary R=chrII.fa O=chrII.dict
java -Xmx2g -jar $EBROOTPICARD/picard.jar CreateSequenceDictionary R=chrIII.fa O=chrII.dict
... etc
```

But let's be lazy and have R submit all the commands for us

```{r eval = FALSE}
# in Unix:

# Load the command modules we'll need. 

module load bwa/0.7.17
module load samtools/1.9
module load picard/2.20.6
module load gcc/7.3.0 nixpkgs/16.09 r/3.6.0 r-bundle-bioconductor/3.9

R

# ---
# in R

# Read the whole genome fasta file to get names of all groups
# Biostrings is much faster than seqinr (but it converts all the bases to upper case)

library(Biostrings)

z <- readDNAStringSet("gasAcu1pitx1.fa", "fasta")
names(z)
	 # [1] "chrI"           "chrII"          "chrIII"         "chrIV"         
	 # [5] "chrIX"          "chrUn"          "chrV"           "chrVI"         
	 # [9] "chrVII"         "chrVIII"        "chrX"           "chrXI"         
	# [13] "chrXII"         "chrXIII"        "chrXIV"         "chrXIX"        
	# [17] "chrXV"          "chrXVI"         "chrXVII"        "chrXVIII"      
	# [21] "chrXX"          "chrXXI"         "chrM"           "chrVIIpitx1new"
	
# Generate bwa index

system("bwa index -a bwtsw gasAcu1pitx1.fa")
for(i in names(z)) system(paste0("bwa index -a bwtsw ", i, ".fa"))

# Generate .fai index files needed by GATK

system("samtools faidx gasAcu1pitx1.fa")
for(i in names(z)) system(paste0("samtools faidx ", i, ".fa"))

# Generate .dict files needed by GATK
# Use "java -Xmx2g" if you requested 2Gb of memory, 
# use "java -Xmx4g" if you requested 4Gb of memory, etc

system("java -Xmx2g -jar $EBROOTPICARD/picard.jar CreateSequenceDictionary R=gasAcu1pitx1.fa O=gasAcu1pitx1.dict")

for(i in names(z)){
	fastafile <- paste0(i, ".fa")
	outfile <- paste0(i, ".dict")
	system(paste0("java -Xmx2g -jar $EBROOTPICARD/picard.jar CreateSequenceDictionary R=", 
			fastafile, " O=", outfile))
	}

q(save = "no")
```

It is **very important** to exit the interactive session if you are done before the session expires. Idle resources and other wastage lowers your future priority in the job queue! Ceiling cat is watching you.

```{r eval = FALSE}
# in Unix

exit
```

<br>

### Show job statistics.

Our interactive job used about 1.25Gb memory

```{r eval = FALSE}
# in Unix

sacct --jobs=26646737 --format=JobID,JobName,AllocCPUS,ExitCode,MaxRSS,Elapsed,UserCPU
	       # JobID    JobName  AllocCPUS ExitCode     MaxRSS    Elapsed    UserCPU 
	# ------------ ---------- ---------- -------- ---------- ---------- ---------- 
	# 26646737             sh          1      0:0              00:27:01  12:43.471 
	# 26646737.ex+     extern          1      0:0       682K   00:27:02   00:00:00 
	# 26646737.0         bash          1      0:0   1253238K   00:26:59  12:43.470 

```

<br>

***

<!-- ========================================================================= -->

## Fastq file reports

Paired reads for an individual will come as two large files in <code>.fastq</code> format (hopefully gzipped, i.e., ```*.fastq.gz```). The first file (file name with "R1" or just "1") will have the forward reads, and the second file (file name with "R2" or "2") will contain reverse complement reads.

<br>

### zless

To view contents of a compressed <code>fastq.gz</code> file, use the unix commands <code>zless</code> and <code>zmore</code> (like <code>less</code> and <code>more</code> for uncompressed text files).

```{r eval = FALSE}
# in unix

zless myFile.fastq.gz
```

<br>

### ShortRead commands

You can use the <code>ShortRead</code> package in R to read <code>fastq.gz</code> files and report on the contents. The <code>fastq.gz</code> files are generally huge, so use ```qa()``` to take a large random sample of reads instead (the manual says the default number of reads is 1 million, but I don't think so).

Use ```report()``` to obtain a report on a file, or use specific commands to obtain a subset of the report.

```{r eval = FALSE}
# in Unix:

# Request resources (here, 1 hour, 1 core, 4Gb memory)
# (copy job allocation number down for later use)
# Wait a few minutes for the resources to be made available

salloc --time=1:0:0 --ntasks-per-node=1 --mem-per-cpu=4G --account=def-schluter

# Load modules and run R

module load gcc/7.3.0 nixpkgs/16.09 r/3.6.0 r-bundle-bioconductor/3.9
R

# In R

library(ShortRead)

fishname <- "samplename.fastq.gz"
myFileName <- dir(getwd(), fishname)

# Large random sample of reads

reads <- qa(myFileName) 

# Total number of reads read

reads[["readCounts"]]   

# Base frequencies in the 1M reads

reads[["baseCalls"]]

# Mean or median base quality

z <- rep(as.integer(rownames(reads[["baseQuality"]])), reads[["baseQuality"]]$count)
mean(z, na.rm = TRUE)
median(z, na.rm = TRUE)

# Plot of read quality by cycle
# Plot will be saved in file Rplot.pdf

perCycle <- reads[["perCycle"]] 
ShortRead:::.plotCycleQuality(perCycle$quality)
dev.off()

# Or produce a full summary report

report(reads, dest = paste(myFileName, "QAreport", sep = "."), type = "html")

q(save = "no")

# in Unix

# Very important to exit the interactive job so resources don't idle

exit
```

The report results will be placed in a folder named after the <code>fastq.gz</code> files analyzed. Download the folder to your local machine and double click the <code>index.html</code> file inside the folder to view all the plots in your browser and print to a pdf.

<br>

### Job statistics

Once you exit, you can view the stats on resources actually used. Substitute the actual job id for [jobid].

```{r eval = FALSE}
# in Unix:

sacct --jobs=[jobid] --format=JobID,JobName,AllocCPUS,ExitCode,MaxRSS,Elapsed,UserCPU
```

***

<!-- ========================================================================= -->

## Align reads with bwa

We use <code>bwa</code> to align paired reads to the reference genome (<code>bwa</code> manual is <a href="http://bio-bwa.sourceforge.net/bwa.shtml">here</a>). The alignments are saved in uncompressed <code>.sam</code> files.

Now we begin to require significant computational resources. Running ```bwa``` for one fish will might take 5 hours of cpu time and require up to 4Gb of memory. So you would need either 1) to run in an interactive session, or 2) to create and submit a bash script to run in batch mode. I show how to do both below. It is worth trying both.

<br>

### Basic bwa command

Firstly, the basic <code>bwa</code> command is as follows (the backslash continues a single command on the next line).

```{r eval = FALSE}
# in unix

bwa mem -M gasAcu1pitx1.fa fishname_R1.fastq.gz \
	    fishname_R2.fastq.gz > fishname.sam
```

But we can speed up this process.

<br>

### Run bwa in parallel

A single ```bwa``` job can be divided up into multiple threads that are run concurrently on separate cores. This greatly speeds things up. The basic command is as follows. Here, the command divides the process among 32 cores, speeding up the whole operation many fold. Most nodes on Graham have 32 cores, so you can go up to 32 threads.

Running this command requires that you request the 32 cores for your job. Try it in an interactive session. Try requesting just 1 hour of total time (```bwa``` on one fish will probably go fast, because the task is split between many cores). The memory across the 32 cores is shared, so you don't need much memory per node. 

(When you request an interactive session, you'll have to wait until the resources become available, which could take 10 minutes or more, depending on demand and the size of your request.)

```{r eval = FALSE}
# in unix

salloc --time=1:0:0 --ntasks-per-node=32 --mem-per-cpu=1G --account=def-schluter

# Wait for the resources to be made available to you.
# Then type the following (substituting the name of your actual fish)

bwa mem -M -t 32 gasAcu1pitx1.fa fishname_R1.fastq.gz \
	    fishname_R2.fastq.gz > fishname.sam

exit
```

<br>

### Submit job script

An interactive session is fine when getting started and you are trying to figure out what resources are needed per fish. But once you've got this figured out you don't want to run each fish one at a time and wait for the process to finish. Instead, submit the job to the scheduler in a script file and go have a coffee.

Using R, I have simplified the task of creating script files. You'll need to run my ```git("genome.r")``` command first, as explained near the top of this page. Here's how it works in the case of a single example fish.

```{r eval = FALSE}
# in unix

module load gcc/7.3.0 nixpkgs/16.09 r/3.6.0 r-bundle-bioconductor/3.9
R

# in R

git <- function(githubfile){
	library(RCurl)
	script <- getURL( paste("https://raw.githubusercontent.com/dschluter/genomeScripts/master/",
	            githubfile, sep="") )
	eval(parse(text = script), envir = .GlobalEnv)
	}
git("genome.r")
git("misc.r")
```

Then put the needed bwa commands into a string object (here named ```bwaCommand```). Note the single quote at the start and end. Also note the double backslash for line continuation when making a string object in R (this will appear as a single backslash in the final bash script file).

```{r eval = FALSE}
bwaCommand <- '
  module load bwa/0.7.17
  bwa mem -M -t 32 gasAcu1pitx1.fa \\
    fishname_R1.fastq.gz \\
    fishname_R2.fastq.gz \\
    > fishname_.sam
	'

# View the command as it will appear in the bash script file
cat(bwaCommand)

# Indicate resources needed for the job
MEM   <- 1     # memory per core, in Gb
TIME  <- 1     # total time in hours
CPUS  <- 32    # number of cores needed

# Generate bash script file using my slurm() function
# Make up an informative prefix, like "myBWAjob"

g$slurm(myCommand, nCpu = CPUS, memPerCpu = MEM, time = TIME, 
	account = "schluter", prefix = "myBWAjob")

q(save = "no")
```

This ends the R session. Now you can view and submit the bash script to the scheduler.

```{r eval = FALSE}
# in unix

# See what the script file looks like

cat [name of your script file]

# submit the script to the scheduler

sbatch [name of your script file]
```

Depending on demand, it might take hours for the job to start execution. So don't watch. Big jobs requesting lots of resources will sit in the queue for longer. 

<br>

### Run many fish

Rather than submit a separate job script for each fish, you might want to submit a job that runs a whole bunch of fish one after another. Here's how to get started. In my code, I'm using 16 threads per fish, which means that with 32 cores I can run 2 fish at the same time. When one fish is completed, the next one in the sequence begins, until all are completed. The management task of running a new fish when one has completed, until all are done, is done by another program called gnu parallel. You'll see how this is coded in the bash script file if you view it before you submit.

```{r eval = FALSE}
# in unix

module load gcc/7.3.0 nixpkgs/16.09 r/3.6.0 r-bundle-bioconductor/3.9
R

# in R

# [paste my git commands here, as you did earlier]

# Put all the bwa commands into a string object.
# Note the single quote at the start and end.

bwaCommand <- '
  module load bwa/0.7.17
  bwa mem -M -t 16 gasAcu1pitx1.fa \\
    fish1.R1.fastq.gz \\
    fish1.R2.fastq.gz \\
    > fish1.sam
  bwa mem -M -t 16 gasAcu1pitx1.fa \\
    fish2.R1.fastq.gz \\
    fish2.R2.fastq.gz \\
    > fish2.sam
  bwa mem -M -t 16 gasAcu1pitx1.fa \\
    fish3.R1.fastq.gz \\
    fish3.R2.fastq.gz \\
    > fish3.sam
  bwa mem -M -t 16 gasAcu1pitx1.fa \\
    fish4.R1.fastq.gz \\
    fish4.R2.fastq.gz \\
    > fish4.sam
    [etc]
	'

# View the command as it will appear in the bash file

cat(bwaCommand)

# Indicate resources needed for the job
MEM   <- 1     # memory per core, in Gb
TIME  <- 12    # total time in hours (depending on number of fish)
CPUS  <- 32    # number of cores needed
JMAX  <- 2     # Run this many fish at a time

# Create the cript file

g$slurm(myCommand, nCpu = CPUS, memPerCpu = MEM, time = TIME, 
	account = "schluter", prefix = "myMultipleBWAjob", gnuJ = JMAX)

q(save = "no")
```

This ends the R session. Now you can view and submit the bash script to the scheduler.

```{r eval = FALSE}
# in unix

# view the script file
cat [name of your bash script file]

# submit the bash script to the scheduler

sbatch [name of your bash script file]

```

<br>

### Check resources used

Check the resoures used using the unix command ```sacct```. 

When I ran a batch of 32 fish with the above script my job id number was 26795176, which I inserted into my ```sacct``` command to obtain the following output.

```{r eval = FALSE}
# in unix

sacct --jobs=26795176 --format=JobID,JobName,AllocCPUS,ExitCode,MaxRSS,Elapsed,UserCPU

	       # JobID    JobName  AllocCPUS ExitCode     MaxRSS    Elapsed    UserCPU 
	# ------------ ---------- ---------- -------- ---------- ---------- ---------- 
	# 26795176     parallel3+         32      0:0              05:26:19 6-05:17:10 
	# 26795176.ba+      batch         32      0:0   5623438K   05:26:19 6-05:17:10 
	# 26795176.ex+     extern         32      0:0       144K   05:26:25   00:00:00 
```

An ExitCode of 0 means that no errors were generated.

MaxRSS is the peak memory use of the job, summed over all cpu’s. Divide by the number of cpu used to get memory use per cpu. This info is useful when planning similar jobs.

```{r eval = FALSE}
5623438000/32
	# [1] 175732438 # = 0.18 Gb
```

UserCPU is cpu time summed over all the cores (cpu's) used. Elapsed is the actual time taken to run the job. Divide UserCPU by Elapsed time and by the number of cores to get efficiency. Aim for an efficiency of 1.

Here, 6-05:17:10 refers to about 6 days and 5 hours when summed across all cores. In decimals, it is 149.29 h. 05:26:19 in decimals is 5.44 h. Efficiency is then calculated as about 0.86. This is good.

```{r eval = FALSE}
149.29/5.44/32
	# [1] 0.8575942
```

See ‘Job accounting fields’ at [https://slurm.schedmd.com/sacct.html](https://slurm.schedmd.com/sacct.html) for a detailed explanation of all items.

<br>

***

<!-- ========================================================================= -->

## GATK pre-processing

Here we take each sam files of aligned reads through the bulk of the GATK4 procedures. We use those steps recommended for "germline short variant discovery (SNPs + Indels)". The output of this sequence of steps is a bam file that is ready for SNP calling.

The steps include the following.
<br>
<br><code>SortSam</code> to sort the reads in the sam file.
<br><code>MarkDuplicates</code> to identify duplicate reads (<b>drop</b> this step if doing GBS).
<br><code>AddOrReplaceReadGroups</code> to assign reads to a single new read group.
<br><code>BaseRecalibrator</code> makes a recalibration table for base quality scores.
<br><code>ApplyBQSR</code> recalibrates the base quality scores.

Base recalibration (last two steps) is optional but we use it in the standard stickleback analysis. You'll need to ensure that the following files are in your current directory (get from Dolph). 
<br>
<br><code>knownSnpsAllchrPitx1new.vcf</code>
<br><code>knownSnpsAllchrPitx1new.vcf.idx</code> (index of previous file)
<br><code>stickleback_21genome_SNP_chrM.bed</code>
<br><code>stickleback_21genome_SNP_chrM.bed.idx</code> (index of previous file)

These files contain lists of known SNPs in the stickleack genome. The file <code>knownSnpsAllchrPitx1new.vcf</code> contains all SNPs discovered in the global survey by Jones et al (2012 Current Biology, https://doi.org/10.1016/j.cub.2011.11.045) using a custom SNP array. Dolph made this file. The file <code>stickleback_21genome_SNP_chrM.bed</code> lists the highest-confidence SNPs discovered in the study of 21 stickleback genomes sampled from marine and freshwater sites around the northern hemisphere by Jones et al (2012 Nature, https://doi.org/10.1038/nature10944). The file was provided by Felicity Jones.

GATK4 does not require realigning reads around indels if using ```HaplotypeCaller``` later to call SNPs.

<br>

### Basic commands

Here's the sequence of commands applied to a single fish whose sam file name is ```GrowthHormoneTransgenicLCR_A-line.GHT-A1.sam```. Some of the commands require a lot of memory. I had to go up to 24Gb to avoid crashes.

```SortSam```, ```MarkDuplicates```, and ```AddOrReplaceReadGroups``` are actually Picard tools, hence the different syntax. The flag ```Xmx8g``` specifies the maximum amount of memory that can be used. Set the number equal to the amount of memory requested for the job (here, 8 Gb in the ```salloc``` command). If your ```salloc``` command requests 12 Gb, then use ```Xmx12g``` in the commands below.

Don't forget to ```exit``` the interaction session at the end so that you aren't sitting on idle computer resources.

```{r eval = FALSE}
# in unix

# Request resources

salloc --time=2:0:0 --ntasks-per-node=1 --mem-per-cpu=24G --account=def-schluter

# You'll have to wait for the resources to become available to continue

module load gatk/4.1.2.0
module load picard/2.20.6
module load java/1.8.0_192 # needed if using gnu parallel

java -Xmx24g -jar $EBROOTPICARD/picard.jar SortSam \
  I=fish1.sam \
  O=fish1.sorted.bam \
  SORT_ORDER=coordinate CREATE_INDEX=TRUE VALIDATION_STRINGENCY=LENIENT

java -Xmx24g -jar $EBROOTPICARD/picard.jar MarkDuplicates \
  I=fish1.sorted.bam \
  O=fish1.mkdup.bam \
  M=fish1.mkdup.metrics \
  VALIDATION_STRINGENCY=LENIENT REMOVE_DUPLICATES=FALSE ASSUME_SORTED=TRUE

java -Xmx24g -jar $EBROOTPICARD/picard.jar AddOrReplaceReadGroups \
  RGID=PROJECT.fish1 \
  RGLB=fish1.SB \
  RGSM=fish1 RGPL=ILLUMINA \
  RGPU=GenomeBC I=fish1.mkdup.bam \
  O=fish1.sorted.bam \
  SORT_ORDER=coordinate CREATE_INDEX=TRUE VALIDATION_STRINGENCY=LENIENT

gatk --java-options "-Xmx24g" BaseRecalibrator \
  -R gasAcu1pitx1.fa -I fish1.sorted.bam \
  --known-sites knownSnpsAllchrPitx1new.vcf \
  --known-sites stickleback_21genome_SNP_chrM.bed \
  -O fish1.recal.table

gatk --java-options "-Xmx24g" ApplyBQSR -R gasAcu1pitx1.fa \
  -I fish1.sorted.bam \
  --bqsr-recal-file fish1.recal.table \
  -O fish1.recal.bam

exit
```

Use the ```sacct``` command to see what resources you actually used and see if the exit code is 0 (indicating all is well).

```{r eval = FALSE}

sacct --jobs=[jobid] --format=JobID,JobName,AllocCPUS,ExitCode,MaxRSS,Elapsed,UserCPU

```

<br>

### Make job script

The interactive session is a good way to figure out how many resources you need. But a more convenient way to run a fish through the sequence of commands is to make a bash script file and submit the job to the scheduler. Here's how to make a script file for a single fish.

```{r eval = FALSE}
# in unix

module load gcc/7.3.0 r/3.6.0 r-bundle-bioconductor/3.9
R

# in R

# Make a character object with basic commands

gatkCommand <- '
module load gatk/4.1.2.0
module load picard/2.20.6
module load java/1.8.0_192 # needed if using gnu parallel

java -Xmx24g -jar $EBROOTPICARD/picard.jar SortSam I=fish1.sam \\
	O=fish1.sorted.bam \\
	SORT_ORDER=coordinate CREATE_INDEX=TRUE VALIDATION_STRINGENCY=LENIENT

java -Xmx24g -jar $EBROOTPICARD/picard.jar MarkDuplicates \\
	I=fish1.sorted.bam \\
	O=fish1.mkdup.bam \\ 
	M=fish1.mkdup.metrics \\
	VALIDATION_STRINGENCY=LENIENT REMOVE_DUPLICATES=FALSE ASSUME_SORTED=TRUE

java -Xmx24g -jar $EBROOTPICARD/picard.jar AddOrReplaceReadGroups \\
	RGID=PROJECT.fish1 \\ 
	RGLB=fish1.SB \\
	RGSM=fish1 RGPL=ILLUMINA \\
	RGPU=PROJECT I=fish1.mkdup.bam \\
	O=fish1.sorted.bam \\
	SORT_ORDER=coordinate CREATE_INDEX=TRUE VALIDATION_STRINGENCY=LENIENT

gatk --java-options "-Xmx24g" BaseRecalibrator \\
	-R gasAcu1pitx1.fa -I fish1.sorted.bam \\
	--known-sites knownSnpsAllchrPitx1new.vcf \\
	--known-sites stickleback_21genome_SNP_chrM.bed \\
	-O fish1.recal.table

gatk --java-options "-Xmx24g" ApplyBQSR -R gasAcu1pitx1.fa \\
	-I fish1.sorted.bam \\
	--bqsr-recal-file fish1.recal.table \\
	-O fish1.recal.bam
'

# View the object

cat(gatkCommand)

# Decide on memory and runtime requirements

MEM   <- 24	# memory per cpu, in Gb
TIME  <- 6	# total time in hours
CPUS  <- 1	# number of cpu's (cores)

# Generate the bash script file

g$slurm(gatkCommand, nCpu = CPUS, memPerCpu = MEM, time = TIME, 
	account = "schluter", prefix = "GatkRun")
```

<br>

### Submit job script

Finally, submit the job to the scheduler. 

Once the job has finished, check what resources were used and whether the exit code was 0.

```{r eval = FALSE}
# in unix

# Submit job

sbatch [script file name]

# Resources actually used

sacct --jobs=[jobid] --format=JobID,JobName,AllocCPUS,ExitCode,MaxRSS,Elapsed,UserCPU

```

<br>

### Run many fish

Rather than submit a separate job script for each fish, you can submit a single script to run a large number of fish one after another. Ask me if you want to try this.

***

<!-- ========================================================================= -->

## Bam quality metrics

The bam files created in the last step contain a lot of information about read pair numbers and coverage along chromosomes. Here's how to use R to extract some of this information.

The bam files can be huge. The code below examines one chromosome at a time. 

You might need to run this code in an interactive session if the memory demands are too great.

```{r eval = FALSE}
# in unix

module load gcc/7.3.0 r/3.6.0 r-bundle-bioconductor/3.9
R

# in R

library(Rsamtools)
library(bitops)
```

<br>

### Choose bamfile and chromosome

Select the bamfile and chromosome to use.

```{r eval = FALSE}

bamfile <- "myBamFile.bam"
chr <- "chrXXI"
baifile <- sub("bam$", "bai", bamfile) # name of bam index file
```

<br>

### Read from bam file

This next block of code reads fields from the bam file (will take a few minutes), drops unaligned reads, and collects aligned reads.

```{r eval = FALSE}

x <- scanBam(bamfile, index = baifile, 
      param = ScanBamParam(what=c("pos","qwidth","flag","strand","isize"),
      which = GRanges(seqnames = chr, IRanges(1, 536870912)) ))[[1]]
ind <- !is.na(x[["pos"]]) 
x <- lapply(x, function(x){x[ind]}) # keep only aligned reads
ranges1 <- IRanges(start=x$pos, width=x$qwidth,
      names=make.names(x[["qname"]], unique=TRUE)) 
```

<br>

### Coverage

Calculate coverage of mapped reads, and plot the frequency distribution of coverage.

```{r eval = FALSE}

cover <- as.vector(coverage(ranges1))
mean(cover) # mean coverage over all bases
median(cover) # median coverage

# If saving plot to file, use dev.off() to close pdf file.

hist(cover[cover <= 100], right = FALSE, breaks = 50, 
    main="Coverage", col="firebrick")
dev.off()

```

<br>

### Number of reads

Number of mapped reads. 

```{r eval = FALSE}

x[["mapped"]] <- bitAnd(x[["flag"]], 0x0004) != 0x0004  
x[["mappedmate"]] <- bitAnd(x[["flag"]], 0x0008) != 0x0008
x[["mapped.in.properpair"]] <- bitAnd(x[["flag"]], 0x0002) == 0x0002

sum(x[["mapped"]])   # No. mapped reads
table(x[["strand"]]) # No. mapped reads on "+" and "-" strands
sum(x[["mapped"]] & x[["mappedmate"]]) # No. reads whose mate also mapped
sum(x[["mapped.in.properpair"]]) # No. reads mapped in a proper pair
```

Plot of position of mapped reads along the chromosome (the 1-based leftmost position of each query on the reference sequence).

```{r eval = FALSE}

hist(x$pos/10^6, right=FALSE, col="firebrick",
    breaks = 200, xlab = "Position (million bases)")
dev.off()
```

<br>

### Other useful numbers

Alignment sequence (query) width.

```{r eval = FALSE}

median(x[["qwidth"]], na.rm = TRUE)
```

Plot of "insert" or "template" length, the number of bases from the leftmost mapped base to the rightmost mapped base. The leftmost segment has a plus sign and the rightmost has a minus sign.

```{r eval = FALSE}

hist(x$isize[abs(x$isize) <= 700], right = FALSE, 
     col = "firebrick", breaks=100)
dev.off()

```


***

<!-- ========================================================================= -->

## HaplotypeCaller (GATK)

This step uses GATK's ```HaplotypeCaller``` to call SNPs for all chromosomes separately on an individual. Each chromosome yields a compressed gVCF file.

Start by typing the basic ```HaplotypeCaller``` commands into a string. The example below uses the option to include all sites in the gVCF file, including both variant and invariant sites. The <code>-A</code> arguments of the HaplotypeCaller command represent requests that specific annotations be included in the output file. FASTAFILE, IDNAME and CHROMOSOME are placeholders, substituted out in a later step.

The commands below were run on a single individual whose recalibrated, compressed bam file (output of the GATK pre-processing steps above) was 7.02 Gb in size. The run took 32 hours, so you want to submit the job to the scheduler (batch mode). 

<br>

### Basic command

```{r eval = FALSE}
# In R

# Put the modules needed at the start of the gnu command string

modules <- '
module load gatk/4.1.2.0
module load java/1.8.0_192
'

# Put base command into a string

myGatkCommand <- '
gatk --java-options "-Xmx4g" HaplotypeCaller \\
	-R gasAcu1pitx1.fa -ERC GVCF \\
	-I FISHID.recal.bam \\
	-O FISHID.CHROMOSOME.g.vcf.gz \\
	-L CHROMOSOME \\
	--pcr-indel-model NONE \\
	--heterozygosity 0.01 --indel-heterozygosity 0.00125 \\
	--output-mode EMIT_ALL_SITES \\
	-A DepthPerAlleleBySample -A FisherStrand -A InbreedingCoeff \\
	-A MappingQuality -A MappingQualityRankSumTest \\
	-A QualByDepth -A ReadPosRankSumTest
'
```

<br>

### Separate chromosomes

It is convenient to run each chromosome separately. To do this, make an object that includes the names of all the chromosomes in the reference genome file.

```{r eval = FALSE}

chr = c("chrI","chrII","chrIII","chrIV","chrIX","chrUn","chrV","chrVI","chrVII",
	"chrVIII","chrX","chrXI","chrXII","chrXIII","chrXIV","chrXIX","chrXV",
	"chrXVI","chrXVII","chrXVIII","chrXX","chrXXI","chrM","chrVIIpitx1new")
```

<br>

### Make and submit script

Finally, create a loop that generates a version of the base command for every chromosome of the individual to be processed and pastes them together into a single string object in R (here named ```gatkJob```). Use ```cat()``` to check the object. Use my ```g$slurm()``` command to create the bash script file and then submit it.

Check the .sh file for errors and submit in a unix command line using <code>sbatch</code>. Check the <code>.out</code> files for errors after execution.


```{r eval = FALSE}

gatkJob <- modules
for(i in 1:length(chr)){
	z <- myGatkCommand
	z <- gsub('FISHID', 'RGID=PROJECT.fish1', z)
	z <- gsub('CHROMOSOME', chr[i], z)
	gatkJob <- paste(gatkJob, z, sep = "\n")
	}

# View the string object created

cat(gatkJob)

# Set the resources needed

MEM   <-  4    # memory per cpu, in Gb
TIME  <- 36    # total time in hours
CPUS  <-  1    # number of cpu's (cores) needed

# Generate the bash script file

g$slurm(gatkJob, nCpu = CPUS, memPerCpu = MEM, time = TIME, 
	account = "schluter", prefix = "gatkHapCall", gnuJ = 1)

# In Unix

sbatch [name of script file]

```

<br>

### Run many fish

Rather than submit a separate job script for each fish, you can submit a single script to run multiple fish in parallel. Ask me if you want to try this.

***

<!-- ========================================================================= -->

## Construct a database

In GATK4, we need to construct a database of the gVCF files using GenomicsDBImport before joint snp calling. 

The database will be constructed in folder that doesn't yet exist or is empty. Another nuance is that that when requesting memory resources we need to ask for a little more than is used by the ```gatk``` command. This is incorporated below.

<br>

### Sample file map

The sample map is a tab-delimited text file with sample name and file names listed line by line. To construct, make a data frame with these two columns. Here is an example data frame named ```myFish``` to analyze the chromosome chrXXI. Create the map file by writing the data frame to a text file (here, named "myfish.chrXXI.map") using tab delimitation and no column or row names.

```{r eval = FALSE}
# In R

myFish

  samplename                        filename
1 Marine-Pac-SaritaBay_BC.SAR-29    Marine-Pac-SaritaBay_BC.SAR-29.chrXXI.g.vcf.gz
2 Solitary-CranbyLake.CRL25         Solitary-CranbyLake.CRL25.chrXXI.g.vcf.gz
3 Marine-Pac-MudLake_AK.ML-39       Marine-Pac-MudLake_AK.ML-39.chrXXI.g.vcf.gz
4 Solitary-TroutLake.TRL-2          Solitary-TroutLake.TRL-2.chrXXI.g.vcf.gz
5 Marine-Pac-DoranPark_CA.CA02-29   Marine-Pac-DoranPark_CA.CA02-29.chrXXI.g.vcf.gz
6 Marine-Pac-MudLake_AK.ML-25       Marine-Pac-MudLake_AK.ML-25.chrXXI.g.vcf.gz
...

write.table(myFish, file = "myfish.chrXXI.map", quote = FALSE, col.names = FALSE, 
        row.names = FALSE, sep = "\t")

```

<br>

### Run GenomicsDBImport

You can try this in interactive mode. I tested this for chrXXI on 33 fish, which took about 35 minutes. I named my new database folder ```chrXXIdb```. 

```{r eval = FALSE}
# in unix

salloc --time=1:0:0 --ntasks-per-node=1 --mem-per-cpu=6G --account=def-schluter

module load gatk/4.1.2.0
module load java/1.8.0_192 # needed if using gnu parallel

gatk --java-options "-Xmx4g" GenomicsDBImport \
	--genomicsdb-workspace-path chrXXIdb \
	--sample-name-map myfish.chrXXI.map \
	-L chrXXI

exit
```

***

<!-- ========================================================================= -->

## Call SNPs with GenotypeGVCFs

Finally, use ```GenotypeGVCFs``` to call snps and invariant sites jointly on the samples in your database. I tested this for chrXXI using the database ```chrXXIdb``` created in the previous step. It is important to indicate the name of your database using ```-V gendb://chrXXIdb```. In this example, the output is sent to a VCF file named ```myFish.chrXXI.vcf.gz```. 

The code below runs GenotypeGVCFs in interactive mode. I used the --include-non-variant-sites flag to save genotyes at both variant (SNPs) and invariant sites. We'll pull out the variants at a later step.

```{r eval = FALSE}
# in unix

salloc --time=3:0:0 --ntasks-per-node=1 --mem-per-cpu=4G --account=def-schluter

module load gatk/4.1.2.0
module load java/1.8.0_192 # needed if using gnu parallel

gatk --java-options "-Xmx4g" GenotypeGVCFs \
  -R gasAcu1pitx1new.fa \
  -V gendb://chrXXIdb \
  -L chrXXI \
  --include-non-variant-sites \
  --max-alternate-alleles 3 \
  --standard-min-confidence-threshold-for-calling 20 \
  -O myFish.chrXXI.vcf.gz

exit
```

<br>

### Extract variants

Use ```SelectVariants``` to pull out the variants and save in a new file. The commands for chrXXI are show below. ```-V``` provides the name of the VCF file, and ```-O``` is the output file name. 

```{r eval = FALSE}
# in unix

module load gatk/4.1.2.0

gatk --java-options "-Xmx2g" SelectVariants \
  -R gasAcu1pitx1new.fa \
  -V myFish.chrXXI.vcf.gz \
  -L chrXXI \
  -O myFish.chrXXI.var.vcf.gz \
  --exclude-non-variants
```

<br>

### Extract invariants

Extracting the invariants, if desired, is similar. These are saved to new files for later analysis.

```{r eval = FALSE}
# in unix

module load gatk/4.1.2.0

gatk --java-options "-Xmx2g" SelectVariants \
  -R gasAcu1pitx1new.fa \
  -V myFish.chrXXI.vcf.gz \
  -L chrXXI \
  -O myFish.chrXXI.var.vcf.gz \
  --select-type-to-include NO_VARIATION
```

***

<!-- ========================================================================= -->

## Variant quality score recalibration

Apply Variant Quality Score Recalibration to help distinguish real SNPs from false positives. This is said to be smarter than simply hard filtering your data set. The recalibrator is first trained using a file from a GATK analysis in the Jones lab of the Broad/Stanford 206 genome dataset. The training set contained a list of "true" SNPs obtained by hard filtering those SNP calls using QD < 2.00, FS > 60.000, MQ < 50.00, MQRankSum < -12.500, ReadPosRankSum < -8.000".

The GATK people recommend that this be done on all chromosomes at once.

<br>

### Gather vcf files

If you called SNPs separately for individual chromosomes you need to combine together the variant files. I ran ```GatherVcfs``` with a batch script.

```{r eval = FALSE}
# in R

chrNnames <- paste0("chr", as.roman(1:21))
chrnames <- c(chrNnames, "chrUn", "chrM", "chrVIIpitx1new")
vcfnames <- paste0("myFish.", chrnames, ".var.vcf.gz")

gatherVcfCommand <- '
  module load picard/2.20.6
  java -Xmx4g -jar $EBROOTPICARD/picard.jar GatherVcfs \\
	I=VCFFILES \\
	O=myFish.var.vcf.gz
'
cmd <- sub("I=VCFFILES", paste0("I=", vcfnames, collapse = " \\\\\n"), gatherVcfCommand)
MEM   <- 4   	# memory per cpu
TIME  <- 6 		# total time in hours
CPUS  <- 1    # number of cpu's (cores) needed
g$slurm(cmd, nCpu = CPUS, memPerCpu = MEM, time = TIME, 
	account = "schluter", prefix = "gatherVCF",
	run = TRUE)
```

<br>

### Train the recalibrator

You'll need to have the following files in your working directory. Get from Dolph if you don't have them yet.

```206sticklebacks_GATKvariants.SNP.filtered.vcf.bgz``` (training set)
```206sticklebacks_GATKvariants.SNP.filtered.vcf.bgz.tbi``` (index)

This file was created in the Jones lab by hard filtering a vcf file 

GATK recommends against using ```-an DP``` when training. They also suggest that if this next step fails, try removing ```-an MQ```.

```{r eval = FALSE}
module load gatk/4.1.2.0
module load r/3.6.0
gatk --java-options "-Xmx4g" VariantRecalibrator \\
	-R gasAcu1pitx1.fa \\
	-V myFish.var.vcf.gz \\
	--resource:dbsnp,known=false,training=true,truth=true,prior=6.0 206sticklebacks_GATKvariants.SNP.filtered.vcf.bgz \\
	-an QD -an FS -an MQ -an MQRankSum -an ReadPosRankSum \\
	-mode SNP \\
	--max-gaussians 4 \\
	-O MarinePac.recal \\
	--tranches-file MarinePac.tranches \\
	--rscript-file MarinePac.plots.R
```

I've had the experience of ```VariantRecalibrator``` succeeding but then the last plotting step failing, with warnings about my R path missing. If this happens, run R and enter the following command. This will generate the plot "output.plots.R.pdf" to visualize the input data and learned model. 

```{r eval = FALSE}
# in R

source("output.plots.R")
```

<br>

### Recalibrate

Finally, apply the filtering rules learned from the training data set to your SNP calls. This step does not remove SNPs from your data set. Instead, it just adds a column named FILTER to the data file that indicates whether the SNP PASSes (is a true SNP) or FAILs (is a false positive) the rules. 

```{r eval = FALSE}
# in unix:

module load gatk/4.1.2.0
gatk --java-options "-Xmx2g" ApplyVQSR \
	-R gasAcu1pitx1.fa \
	-V myFish.var.vcf.gz \
	-O myFish.VQSR.vcf.gz \
	--truth-sensitivity-filter-level 99.0 \
	--tranches-file myFish.tranches \
	--recal-file myFish.recal \
	-mode SNP

```

<br>

### Hard-filter instead

What are your options if you have no training data? For example, we have no training data set for indels. GATK recommends hard filtering using the tool ```VariantFiltration```. Or, you can filter using a custom R script.

To begin, either remove indels or split your VCF file into two separate files, one containing indels and the other containing SNPs using the Picard tool SplitVcfs.

Recommendations for hard filtering SNPs and indels [are here](https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering).

GATK suggests the following thresholds to label SNPs as PASS or not.

```{r eval = FALSE}
# in unix:
gatk VariantFiltration \
    -V snps.vcf.gz \
    -filter "QD < 2.0" --filter-name "QD2" \
    -filter "QUAL < 30.0" --filter-name "QUAL30" \
    -filter "SOR > 3.0" --filter-name "SOR3" \
    -filter "FS > 60.0" --filter-name "FS60" \
    -filter "MQ < 40.0" --filter-name "MQ40" \
    -filter "MQRankSum < -12.5" --filter-name "MQRankSum-12.5" \
    -filter "ReadPosRankSum < -8.0" --filter-name "ReadPosRankSum-8" \
    -O snps_filtered.vcf.gz
```

If a record fails according to these filters, then the FILTER column in the output VCF file will indicate how;, e.g. MQRankSum-12.5;ReadPosRankSum-8.

To hard filter indels, they recommend the following filters.

```{r eval = FALSE}
# in unix:
gatk VariantFiltration \ 
    -V indels.vcf.gz \ 
    -filter "QD < 2.0" --filter-name "QD2" \
    -filter "QUAL < 30.0" --filter-name "QUAL30" \
    -filter "FS > 200.0" --filter-name "FS200" \
    -filter "ReadPosRankSum < -20.0" --filter-name "ReadPosRankSum-20" \ 
    -O indels_filtered.vcf.gz
```


***

## Additional SNP filters {#filter}

Once GATK has finished calling SNPs, you can filter your data to select the SNPs you are most interested in and delete the rest. Here are some of the filters you might consider: 

- Keep pure SNPs only (drop indels)
- Keep only biallelic SNPs 
- Keep only SNPs that passed VQSR
- Drop SNPs located at repeat-masked bases
- Drop SNPs located within 3 bases of an indel
- Drop SNPs with excessively low or high depth of coverage

At a later step, once you've grouped specimens into populations, you might also want to

- Leave out of some analyses SNPs having a minor allele that is rare (< 5%)
- Analyze only SNPs having at least 80% of individuals genotyped
- Leave out SNPs with excess heterozygsity within populations
- Leave out the sex chromosome
- Leave out mtDNA

***

## Other useful tips {#other}

<br>

### Interactive mode {#interactive}

Test part of your code in interactive mode to determine resources required, and to ensure that it works, before you start analyzing large numbers of samples.

For example, here's how to run <code>bwa</code> on a single sample with multi-threading (<code>-t</code> option) on 16 cpu's for 1 hour. Experience suggests that <code>bwa</code> doesn't require much memory per cpu when the job is distributed across 16 cpu. In general, if a program crashes soon after it begins with little explanation of what went wrong, exit interactive mode and restart requesting more memory.

Include the "exit" command when you paste text to the command line so that your interactive job terminates after your work is done. That way no computer resources sit idle for a time afterward (which causes your priority rating to go down).

Once you have exited interactive mode, determine resources used (see Monitor jobs, below).

```{r eval = FALSE}
# in unix

# To begin, you need to reserve a fixed amount of computer resources. 
# Be ungenerous, Compute Canada charges for unused resources, lowering your priority score
salloc --time=1:0:0 --ntasks-per-node=16 --mem-per-cpu=1G --account=def-schluter

# Load the bwa module and run bwa (single backslash continues a command in unix)
module load bwa/0.7.15
bwa mem -M -t 16 gasAcu1pitx1new.fa Marine-Pac-Salmon-01-Sara_R1.fastq.gz \
	Marine-Pac-Salmon-01-Sara_R2.fastq.gz < Marine-Pac-Salmon-01-Sara.sam
exit
```


<br>

### Parallel processing in interactive mode
Gnu parallel can run multiple serial jobs simultaneously on different cpu's (cores) of the same node. Up to 32 cpu's are available per node.

For example, here we use gnu parallel in interactive mode to run <code>bwa</code> on two individuals (no multi-threading [no "-t" argument to <code>bwa</code>] for the purposes of this example).

```{r eval = FALSE}
# in unix

# Request two cpu's for 3 hours and start interactive job
salloc --time=3:0:0 --ntasks-per-node=2 --mem-per-cpu=2G --account=def-schluter

# Load bwa module
module load bwa/0.7.15

# run bwa on two samples, each on a separate cpu, no multi-threading
parallel -j 2 <<gnu
bwa mem -M gasAcu1pitx1new.fa Marine-Pac-Salmon-01-Sara_R1.fastq.gz Marine-Pac-Salmon-01-Sara_R2.fastq.gz < Marine-Pac-Salmon-01-Sara.sam
bwa mem -M gasAcu1pitx1new.fa Marine-Pac-Seyward-01-Sara_R1.fastq.gz Marine-Pac-Seyward-01-Sara_R2.fastq.gz < Marine-Pac-Seyward-01-Sara.sam
gnu
exit
```


<br>

### Submit batch jobs {#submit}
Run large jobs in batch mode. This involves writing a <code>bash</code> script file (<code>filename.sh</code>) that contains the commands you want run. The script file is then submitted to the <code>slurm</code> job scheduler. Then go do something else while the job is queued and run.

The example below requests 16 cpu's for a single <code>bwa</code> command. In general, Under Graham's job priority system, it is best to submit jobs that make use of a whole node (32 cpu's).

```{r eval = FALSE}
# in R

# Put job commands into an R string (between single quotes, ' ')
# In R string, use double backslash '\\' to continue a command on multiple lines
# Include commands to load modules needed
bwaCommand <- '
	module load bwa/0.7.15
	bwa mem -M -t 16 gasAcu1pitx1new.fa Marine-Pac-Salmon-01-Sara_R1.fastq.gz \\
		Marine-Pac-Salmon-01-Sara_R2.fastq.gz < Marine-Pac-Salmon-01-Sara.sam
	' 
# Check
cat(bwaCommand)

# Choose resources needed for the job
MEM  <- 1     # memory per cpu, in Gb
TIME <- 1     # total time in hours
CPUS <- 16    # number of cpu's (cores) needed

# Generate bash script file
# filename.sh includes date in name to make unique.
# Use an informative prefix. 
g$slurm(bwaCommand, nCpu = CPUS, memPerCpu = MEM, time = TIME, 
	account = "schluter", prefix = "bwa", run = FALSE)
```

R will create the <code>filename.sh</code> file, which you can inspect to confirm that it is constructed correctly. Then submit on a unix command lie with <code>sbatch filename.sh</code>. You'll need to exit R or open a second terminal window to run the <code>sbatch</code> command.

<br>

### Gnu parallel batch jobs {#parallel}
Use <code>gnu parallel</code> to run multiple serial commands as a single parallel job. The homemade R function <code>g$slurm()</code> automates creation of a script file for the <code>slurm</code> scheduler.

You need to specify the number of tasks to run at a time (in parallel). In the example below, we run just two <code>bwa</code> commands that each use 16 cpu's (16 threads are used by each <code>bwa</code> command). Since there are only 32 cpu's in a node, we can run at most 2 <code>bwa</code> commands at the same time. (You can list more than two commands; <code>gnu parallel</code> waits until one of the runs is finished to begin executing the next command in the list.)

```{r eval = FALSE}
# in R

# Put commands into an R string (between single quotes, ' ')
# In R string, use double backslash '\\' to continue a single command on multiple lines
# Include commands to load modules needed
gnuCommand <- '
	module load bwa/0.7.15
	bwa mem -M -t 16 gasAcu1pitx1new.fa Marine-Pac-Salmon-01-Sara_R1.fastq.gz \\
		Marine-Pac-Salmon-01-Sara_R2.fastq.gz < 1.sam
	bwa mem -M -t 16 gasAcu1pitx1new.fa Marine-Pac-Seyward-01-Sara_R1.fastq.gz \\
		Marine-Pac-Seyward-01-Sara_R2.fastq.gz < 2.sam
	'

# Choose resources needed for the job
MEM   <- 1     # memory per cpu, in Gb
TIME  <- 1     # total time in hours
CPUS  <- 32    # number of cpu's (cores) needed
JMAX  <- 2     # Run this many commands at a time

# Generate bash script file
# filename.sh includes date in name to make unique.
# Use an informative prefix. 
g$slurm(gnuCommand, nCpu = CPUS, memPerCpu = MEM, time = TIME, 
	account = "schluter", prefix = "gnuBwa", gnuJ = JMAX)
```

R will generate the script (<code>.sh</code>) file, which you can submit to the job scheduler using

```{r eval = FALSE}
# in unix
sbatch [name of script file]
```


<br>

### Monitor jobs {#monitor}
To check on the status of all jobs submitted to the scheduler, type the following into a unix command window (including your login name). If your job is missing, then it has run and will not be in the queue any longer.

```{r eval = FALSE}
# in unix
squeue -u [userid]
```

To cancel a job, type the following into a unix command window. "[jobid]" is a number assigned to your job when you submitted it to the scheduler.

```{r eval = FALSE}
scancel [jobid]
```

Each job will generate an <code>.out</code> file when your code completes. Manually check it for errors. An exit code of 0 means that no errors were generated

Check on the resources used by your job when it is done.

```{r eval = FALSE}
sacct --jobs=[jobid] --format=JobID,JobName,AllocCPUS,ExitCode,MaxRSS,Elapsed,UserCPU
```

An ExitCode of 0 means that no errors were generated.

MaxRSS is the peak memory use of the job, summed over all cpu's. Divide by the number of cpu used to get memory use per cpu. This amount is useful when planning similar jobs.

UserCPU is cpu time summed over all the cpu's used. Elapsed is the actual time taken to run the job. Divide UserCPU by Elapsed to get the efficiency. Aim for high efficiency.

See 'Job accounting fields' at https://slurm.schedmd.com/sacct.html for a detailed explanation of all items.

Use the following to check on the resources used by all your jobs submitted between two dates.

```{r eval = FALSE}
sacct --user=[userid] --starttime=yyyy-mm-dd --endtime=yyyy-mm-dd \
  --format=JobID,JobName,AllocCPUS,ExitCode,MaxRSS,Elapsed,UserCPU

# filter those jobs containing 'bwa' in the JobName
sacct --user=[userid] --starttime=yyyy-mm-dd --endtime=yyyy-mm-dd \
  --format=JobID,JobName,AllocCPUS,ExitCode,MaxRSS,Elapsed,UserCPU \
  | grep -i 'bwa' -A1
```


